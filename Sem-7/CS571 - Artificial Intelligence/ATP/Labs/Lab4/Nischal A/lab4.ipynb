{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1>CS561 - ARTIFICIAL INTELLIGENCE LAB</h1>\r\n",
    "<h1>ASSIGNMENT-4: DECISION TREE</h1>\r\n",
    "\r\n",
    "<b><u>Group_Name</u></b>: <a style=\"font-size:15px\">1801cs31_1801cs32_1801cs33</a><br>\r\n",
    "<b><u>Date</u></b>: 04-Oct-2021\r\n",
    "\r\n",
    "<table style=\"font-size:15px\">\r\n",
    "    <thead>\r\n",
    "        <td><b>Name of Student</b></td>\r\n",
    "        <td><b>Roll No.</b></td>\r\n",
    "        <td><b>Batch</b></td>\r\n",
    "    </thead>\r\n",
    "    <tr>\r\n",
    "        <td>M Maheeth Reddy</td>\r\n",
    "        <td>1801CS31</td>\r\n",
    "        <td>B.Tech.</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "        <td>M Nitesh Reddy</td>\r\n",
    "        <td>1801CS32</td>\r\n",
    "        <td>B.Tech.</td>\r\n",
    "    </tr>\r\n",
    "    <tr>\r\n",
    "        <td>Nischal A</td>\r\n",
    "        <td>1801CS33</td>\r\n",
    "        <td>B.Tech.</td>\r\n",
    "    </tr>\r\n",
    "</table>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import string\n",
    "import operator\n",
    "from copy import deepcopy\n",
    "from math import log2\n",
    "from statistics import mean\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/maheeth/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/maheeth/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/maheeth/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def get_ngrams(data, n):\n",
    "    tokens = [token for token in data.split(\" \") if token != \"\"]\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "def get_postag(txt):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenized = sent_tokenize(txt)\n",
    "    words_list = nltk.word_tokenize(tokenized[0]) \n",
    "    words_list = [w for w in words_list if not w in stop_words]  \n",
    "    return nltk.pos_tag(words_list)\n",
    "\n",
    "def build_data(path_to_data):\n",
    "    data,uni,bi,tri,pos = [],[],[],[],[]\n",
    "    file = open(path_to_data)\n",
    "\n",
    "    for line in file:\n",
    "        line = line.split(':')\n",
    "        row = []\n",
    "        \n",
    "        _class, _question = line[0], line[1]\n",
    "        row.append(_class)\n",
    "        row.append(' '.join(_question.split(' ')[1:]).translate(str.maketrans('', '', string.punctuation)).rstrip())\n",
    "\n",
    "        length = len(row[1].split(' '))\n",
    "        row.append(length)\n",
    "\n",
    "        unigram = get_ngrams(row[1], 1)\n",
    "        row.append(unigram)\n",
    "        uni.extend(unigram)\n",
    "\n",
    "        bigram = get_ngrams(row[1], 2)\n",
    "        row.append(bigram)\n",
    "        bi.extend(bigram)\n",
    "        \n",
    "        trigram = get_ngrams(row[1], 3)\n",
    "        row.append(trigram)\n",
    "        tri.extend(trigram)\n",
    "\n",
    "        postag = get_postag(row[1])\n",
    "        row.append(postag)\n",
    "        pos.extend(postag)\n",
    "\n",
    "        # complete set of features for each text \n",
    "        data.append(row)\n",
    "\n",
    "    return data, uni, bi, tri, pos\n",
    "\n",
    "# load training data\n",
    "data, uni, bi, tri, pos = build_data('./train_data.txt')\n",
    "print('Loading Training data...')\n",
    "print('Training Data:')\n",
    "print(data[0:5])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Training data...\n",
      "Training Data:\n",
      "[['DESC', 'How did serfdom develop in and then leave Russia', 9, [('How',), ('did',), ('serfdom',), ('develop',), ('in',), ('and',), ('then',), ('leave',), ('Russia',)], [('How', 'did'), ('did', 'serfdom'), ('serfdom', 'develop'), ('develop', 'in'), ('in', 'and'), ('and', 'then'), ('then', 'leave'), ('leave', 'Russia')], [('How', 'did', 'serfdom'), ('did', 'serfdom', 'develop'), ('serfdom', 'develop', 'in'), ('develop', 'in', 'and'), ('in', 'and', 'then'), ('and', 'then', 'leave'), ('then', 'leave', 'Russia')], [('How', 'WRB'), ('serfdom', 'JJ'), ('develop', 'VB'), ('leave', 'JJ'), ('Russia', 'NNP')]], ['ENTY', 'What films featured the character Popeye Doyle', 7, [('What',), ('films',), ('featured',), ('the',), ('character',), ('Popeye',), ('Doyle',)], [('What', 'films'), ('films', 'featured'), ('featured', 'the'), ('the', 'character'), ('character', 'Popeye'), ('Popeye', 'Doyle')], [('What', 'films', 'featured'), ('films', 'featured', 'the'), ('featured', 'the', 'character'), ('the', 'character', 'Popeye'), ('character', 'Popeye', 'Doyle')], [('What', 'WP'), ('films', 'VBD'), ('featured', 'JJ'), ('character', 'NN'), ('Popeye', 'NNP'), ('Doyle', 'NNP')]], ['DESC', 'How can I find a list of celebrities  real names', 11, [('How',), ('can',), ('I',), ('find',), ('a',), ('list',), ('of',), ('celebrities',), ('real',), ('names',)], [('How', 'can'), ('can', 'I'), ('I', 'find'), ('find', 'a'), ('a', 'list'), ('list', 'of'), ('of', 'celebrities'), ('celebrities', 'real'), ('real', 'names')], [('How', 'can', 'I'), ('can', 'I', 'find'), ('I', 'find', 'a'), ('find', 'a', 'list'), ('a', 'list', 'of'), ('list', 'of', 'celebrities'), ('of', 'celebrities', 'real'), ('celebrities', 'real', 'names')], [('How', 'WRB'), ('I', 'PRP'), ('find', 'VBP'), ('list', 'JJ'), ('celebrities', 'NNS'), ('real', 'JJ'), ('names', 'NNS')]], ['ENTY', 'What fowl grabs the spotlight after the Chinese Year of the Monkey', 12, [('What',), ('fowl',), ('grabs',), ('the',), ('spotlight',), ('after',), ('the',), ('Chinese',), ('Year',), ('of',), ('the',), ('Monkey',)], [('What', 'fowl'), ('fowl', 'grabs'), ('grabs', 'the'), ('the', 'spotlight'), ('spotlight', 'after'), ('after', 'the'), ('the', 'Chinese'), ('Chinese', 'Year'), ('Year', 'of'), ('of', 'the'), ('the', 'Monkey')], [('What', 'fowl', 'grabs'), ('fowl', 'grabs', 'the'), ('grabs', 'the', 'spotlight'), ('the', 'spotlight', 'after'), ('spotlight', 'after', 'the'), ('after', 'the', 'Chinese'), ('the', 'Chinese', 'Year'), ('Chinese', 'Year', 'of'), ('Year', 'of', 'the'), ('of', 'the', 'Monkey')], [('What', 'WP'), ('fowl', 'NN'), ('grabs', 'NN'), ('spotlight', 'NN'), ('Chinese', 'JJ'), ('Year', 'NNP'), ('Monkey', 'NNP')]], ['ABBR', 'What is the full form of com', 7, [('What',), ('is',), ('the',), ('full',), ('form',), ('of',), ('com',)], [('What', 'is'), ('is', 'the'), ('the', 'full'), ('full', 'form'), ('form', 'of'), ('of', 'com')], [('What', 'is', 'the'), ('is', 'the', 'full'), ('the', 'full', 'form'), ('full', 'form', 'of'), ('form', 'of', 'com')], [('What', 'WP'), ('full', 'JJ'), ('form', 'NN'), ('com', 'NN')]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def top_grams(grams, top_n):\n",
    "    return Counter(grams).most_common(top_n)\n",
    "\n",
    "unigram_counts = top_grams(uni, 500)\n",
    "bigram_counts = top_grams(bi, 300)\n",
    "trigram_counts = top_grams(tri, 200)\n",
    "pos_counts = top_grams(pos, 500)\n",
    "\n",
    "avg_length = mean([row[2] for row in data])\n",
    "print('average length:',avg_length)\n",
    "\n",
    "# Displaying the top features\n",
    "print('Top features:\\n')\n",
    "print('Unigrams:\\n')\n",
    "print(unigram_counts[0:5])\n",
    "\n",
    "print('Bigrams:\\n')\n",
    "print(bigram_counts[0:5])\n",
    "\n",
    "print('Trigrams:\\n')\n",
    "print(trigram_counts[0:5])\n",
    "\n",
    "#\n",
    "print('Pos Counts:\\n')\n",
    "print(pos_counts[0:5])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "average length: 9.031548055759353\n",
      "Top features:\n",
      "\n",
      "Unigrams:\n",
      "\n",
      "[(('the',), 3589), (('What',), 3245), (('is',), 1669), (('of',), 1540), (('in',), 1131)]\n",
      "Bigrams:\n",
      "\n",
      "[(('What', 'is'), 968), (('is', 'the'), 757), (('of', 'the'), 446), (('in', 'the'), 326), (('How', 'many'), 316)]\n",
      "Trigrams:\n",
      "\n",
      "[(('What', 'is', 'the'), 551), (('What', 'is', 'a'), 151), (('What', 's', 'the'), 135), (('What', 'are', 'the'), 134), (('What', 'was', 'the'), 130)]\n",
      "Pos Counts:\n",
      "\n",
      "[(('What', 'WP'), 3245), (('How', 'WRB'), 763), (('Who', 'WP'), 559), (('many', 'JJ'), 332), (('Where', 'WRB'), 273)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def is_numeric(value):\n",
    "    return isinstance(value, int) or isinstance(value, float)\n",
    "\n",
    "header = ['Label', 'Text', 'Length', 'Unigram', 'Bigram', 'Trigram', 'POS']\n",
    "\n",
    "class Question:\n",
    "    def __init__(self, col, value):\n",
    "        self.col = col # The column number in the header\n",
    "        self.value = value # Actual value of the object\n",
    "\n",
    "    # Matching attributes of current question with the current row\n",
    "    def match(self, example):\n",
    "        val = example[self.col]\n",
    "        if is_numeric(val):\n",
    "            return val <= self.value\n",
    "        \n",
    "        return self.value in val\n",
    "\n",
    "    # Return the string representation of the object\n",
    "    def __repr__(self):\n",
    "        condition = \"contains\"\n",
    "        return \"Does %s %s %s?\" % (\n",
    "            header[self.col], condition, str(self.value))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def class_counts(rows):\n",
    "    counts = {}\n",
    "    for row in rows:\n",
    "        label = row[0]\n",
    "\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        \n",
    "        counts[label] += 1\n",
    "    return counts\n",
    "\n",
    "def gini(rows):\n",
    "    counts = class_counts(rows)\n",
    "    impurity = 1\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        impurity -= prob_of_lbl**2\n",
    "    return impurity\n",
    "\n",
    "def misclassifcation_error(rows):\n",
    "    counts = class_counts(rows)\n",
    "    max_prob = 0\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        if prob_of_lbl > max_prob:\n",
    "            max_prob = prob_of_lbl\n",
    "    return 1 - max_prob\n",
    "\n",
    "def entropy(rows):\n",
    "    counts = class_counts(rows)\n",
    "    impurity = 0\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        impurity -= prob_of_lbl*log2(prob_of_lbl)\n",
    "    return impurity\n",
    "\n",
    "def info_gain(left, right, current_uncertainty, func):\n",
    "    p = float(len(left))/(len(left)+len(right))\n",
    "    return current_uncertainty - p*func(left) - (1-p)*func(right)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class Leaf:\n",
    "    def __init__(self, rows):\n",
    "        self.predictions = class_counts(rows)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class Decision_Node:\n",
    "    def __init__(self, question, true_branch, false_branch):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "questions = []\n",
    "\n",
    "for x in unigram_counts:\n",
    "    questions.append(Question(3, x[0]))\n",
    "\n",
    "for x in bigram_counts:\n",
    "    questions.append(Question(4, x[0]))\n",
    "    \n",
    "for x in trigram_counts:\n",
    "    questions.append(Question(5, x[0]))\n",
    "\n",
    "for x in pos_counts:\n",
    "    questions.append(Question(6, x[0]))\n",
    "    \n",
    "questions.append(Question(2, avg_length))    \n",
    "    \n",
    "print(len(questions))\n",
    "print(questions[0:5])\n",
    "# print(questions[1500])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1501\n",
      "[Does Unigram contains ('the',)?, Does Unigram contains ('What',)?, Does Unigram contains ('is',)?, Does Unigram contains ('of',)?, Does Unigram contains ('in',)?]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Returns a split of true rows and false rows for a particular question (single feature)\n",
    "def partition(rows, question):\n",
    "    rows_true = []\n",
    "    rows_false = []\n",
    "    \n",
    "    for r in rows:\n",
    "        if question.match(r):\n",
    "            rows_true.append(r)\n",
    "        else:\n",
    "            rows_false.append(r)\n",
    "    \n",
    "    return rows_true, rows_false"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def find_best_split(rows, questions, func):   \n",
    "    best_gain = 0\n",
    "    best_question = None\n",
    "    current_uncertainty = func(rows)\n",
    "    \n",
    "    for q in questions:\n",
    "        rows_true, rows_false = partition(rows, q)\n",
    "        if len(rows_true) == 0 or len(rows_false) == 0:\n",
    "            continue\n",
    "        \n",
    "        gain = info_gain(rows_true, rows_false, current_uncertainty, func) # Calculating the information gain\n",
    "        # Updating best gain\n",
    "        if gain >= best_gain:\n",
    "            best_gain, best_question = gain, q\n",
    "    \n",
    "    return best_gain, best_question   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Recursive Function to form the decision tree\n",
    "# using partitioning (question list is updated periodically)\n",
    "def form_tree(rows, questions, func):\n",
    "    # Find the best gain and best question\n",
    "    gain, question = find_best_split(rows, questions, func)\n",
    "    if gain == 0:\n",
    "        return Leaf(rows)\n",
    "    \n",
    "    rows_true, rows_false = partition(rows, question)\n",
    "    questions.remove(question)\n",
    "    \n",
    "    true_branch = form_tree(rows_true, questions, func)\n",
    "    false_branch = form_tree(rows_false, questions, func)\n",
    "    \n",
    "    return Decision_Node(question, true_branch, false_branch)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def classify_row(node, row):\n",
    "    if isinstance(node, Leaf):\n",
    "        return node.predictions\n",
    "    \n",
    "    if node.question.match(row):\n",
    "        return classify_row(node.true_branch, row)\n",
    "    else:\n",
    "        return classify_row(node.false_branch, row)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def train(data, questions, func):\n",
    "    return form_tree(data, deepcopy(questions), func)\n",
    "\n",
    "def classify(root, rows):\n",
    "    predictions = [max(classify_row(root, r).items(), key=operator.itemgetter(1))[0] for r in rows]\n",
    "    return predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def get_data_in_index(data, index):\n",
    "    l = []\n",
    "    for i in range(len(data)):\n",
    "        if i in index:\n",
    "            l.append(data[i])\n",
    "    return l\n",
    "\n",
    "def get_actual_labels(act_data):\n",
    "    act_labels = []\n",
    "    \n",
    "    for d in act_data:\n",
    "        act_labels.append(d[0])\n",
    "    \n",
    "    return act_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "kfold = KFold(10, True, 1)\n",
    "precision,recall,f_score = [],[],[]\n",
    "i = 0\n",
    "\n",
    "for trainInd, testInd in kfold.split(data):\n",
    "    train_data = get_data_in_index(data, trainInd)\n",
    "    test_data = get_data_in_index(data, testInd)\n",
    "    \n",
    "    root = train(train_data, questions, gini)\n",
    "    prediction = classify(root, test_data)\n",
    "    actual = get_actual_labels(test_data)\n",
    "    predicted = prediction\n",
    "    \n",
    "    precision.append(precision_score(actual, predicted, average='macro'))\n",
    "    recall.append(recall_score(actual, predicted, average='macro'))\n",
    "    f_score.append(f1_score(actual, predicted, average='macro'))\n",
    "     \n",
    "    print(\"Training...\")\n",
    "\n",
    "print('\\nGini Index')\n",
    "print(\"Precision Score = \"+str(mean(precision)))\n",
    "print(\"Recall Score = \"+str(mean(recall)))\n",
    "print(\"F Score = \"+str(mean(f_score)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/maheeth/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=1 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training...\n",
      "Training...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2\n",
    "- All\n",
    "- Unigram, Bigram, Trigram, POS\n",
    "- Unigram, Bigram, Trigram"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "classes = ['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM']\n",
    "\n",
    "def getReport(train_data, test_data, uniFlag=True, biFlag=True, triFlag=True, posFlag=True, lenFlag=True, func=gini):\n",
    "    allQuestions = []\n",
    "    \n",
    "    if uniFlag:\n",
    "        for x in unigram_counts:\n",
    "            allQuestions.append(Question(3, x[0]))\n",
    "\n",
    "    if biFlag:\n",
    "        for x in bigram_counts:\n",
    "            allQuestions.append(Question(4, x[0]))\n",
    "\n",
    "    if triFlag:\n",
    "        for x in trigram_counts:\n",
    "            allQuestions.append(Question(5, x[0]))\n",
    "\n",
    "    if posFlag:\n",
    "        for x in pos_counts:\n",
    "            allQuestions.append(Question(6, x[0]))\n",
    "\n",
    "    if lenFlag:\n",
    "        allQuestions.append(Question(2, avg_length))    \n",
    "\n",
    "    print(\"No of questions = \" + str(len(allQuestions)))\n",
    "\n",
    "    print(\"Training...\")\n",
    "    root = train(train_data, allQuestions, func)\n",
    "    \n",
    "    print(\"Predicting...\")\n",
    "    prediction = classify(root, test_data)        \n",
    "    actual = get_actual_labels(test_data)\n",
    "    \n",
    "    print(\"Prediction done...\")\n",
    "    matrix = confusion_matrix(actual, prediction)\n",
    "    class_report = classification_report(actual, prediction)\n",
    "    acc = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    accuracy_report = dict(zip(classes, acc))\n",
    "    \n",
    "    return accuracy_report, class_report, root, prediction, actual\n",
    "\n",
    "test_data = build_data('./test_data.txt')[0]\n",
    "print(len(test_data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "500\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "accuracy_report, class_report, root, prediction, actual = getReport(train_data=data, test_data=test_data)\n",
    "print(accuracy_report)\n",
    "print(class_report)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No of questions = 1501\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "{'ABBR': 0.6666666666666666, 'DESC': 0.9710144927536232, 'ENTY': 0.723404255319149, 'HUM': 0.8461538461538461, 'LOC': 0.7037037037037037, 'NUM': 0.8141592920353983}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.86      0.67      0.75         9\n",
      "        DESC       0.76      0.97      0.85       138\n",
      "        ENTY       0.68      0.72      0.70        94\n",
      "         HUM       0.92      0.85      0.88        65\n",
      "         LOC       0.89      0.70      0.79        81\n",
      "         NUM       0.99      0.81      0.89       113\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.85      0.79      0.81       500\n",
      "weighted avg       0.84      0.82      0.82       500\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "accuracy_report, class_report, root, prediction, actual = getReport(train_data=data, test_data=test_data, func=entropy)\n",
    "print(accuracy_report)\n",
    "print(class_report)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No of questions = 1501\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "{'ABBR': 0.6666666666666666, 'DESC': 0.9710144927536232, 'ENTY': 0.5, 'HUM': 0.8615384615384616, 'LOC': 0.7283950617283951, 'NUM': 0.8053097345132744}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.86      0.67      0.75         9\n",
      "        DESC       0.66      0.97      0.79       138\n",
      "        ENTY       0.69      0.50      0.58        94\n",
      "         HUM       0.90      0.86      0.88        65\n",
      "         LOC       0.88      0.73      0.80        81\n",
      "         NUM       0.98      0.81      0.88       113\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.83      0.76      0.78       500\n",
      "weighted avg       0.81      0.79      0.78       500\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "accuracy_report, class_report, root, prediction, actual = getReport(train_data=data, test_data=test_data, func=misclassifcation_error)\n",
    "print(accuracy_report)\n",
    "print(class_report)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No of questions = 1501\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "{'ABBR': 0.6666666666666666, 'DESC': 0.8260869565217391, 'ENTY': 0.7978723404255319, 'HUM': 0.8461538461538461, 'LOC': 0.691358024691358, 'NUM': 0.7876106194690266}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.86      0.67      0.75         9\n",
      "        DESC       0.77      0.83      0.79       138\n",
      "        ENTY       0.57      0.80      0.66        94\n",
      "         HUM       0.92      0.85      0.88        65\n",
      "         LOC       0.92      0.69      0.79        81\n",
      "         NUM       0.98      0.79      0.87       113\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.83      0.77      0.79       500\n",
      "weighted avg       0.82      0.79      0.80       500\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "accuracy_report, class_report, root, prediction, actual = getReport(train_data=data, test_data=test_data, lenFlag=False)\n",
    "print(accuracy_report)\n",
    "print(class_report)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No of questions = 1500\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "{'ABBR': 0.6666666666666666, 'DESC': 0.9710144927536232, 'ENTY': 0.723404255319149, 'HUM': 0.8461538461538461, 'LOC': 0.7037037037037037, 'NUM': 0.8141592920353983}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.86      0.67      0.75         9\n",
      "        DESC       0.76      0.97      0.85       138\n",
      "        ENTY       0.68      0.72      0.70        94\n",
      "         HUM       0.92      0.85      0.88        65\n",
      "         LOC       0.89      0.70      0.79        81\n",
      "         NUM       0.99      0.81      0.89       113\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.85      0.79      0.81       500\n",
      "weighted avg       0.84      0.82      0.82       500\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "accuracy_report, class_report, root, prediction, actual = getReport(train_data=data, test_data=test_data, lenFlag=False, func=entropy)\n",
    "print(accuracy_report)\n",
    "print(class_report)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No of questions = 1500\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "{'ABBR': 0.6666666666666666, 'DESC': 0.9710144927536232, 'ENTY': 0.5, 'HUM': 0.8615384615384616, 'LOC': 0.7283950617283951, 'NUM': 0.8053097345132744}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.86      0.67      0.75         9\n",
      "        DESC       0.66      0.97      0.79       138\n",
      "        ENTY       0.69      0.50      0.58        94\n",
      "         HUM       0.90      0.86      0.88        65\n",
      "         LOC       0.88      0.73      0.80        81\n",
      "         NUM       0.98      0.81      0.88       113\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.83      0.76      0.78       500\n",
      "weighted avg       0.81      0.79      0.78       500\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "accuracy_report, class_report, root, prediction, actual = getReport(train_data=data, test_data=test_data, lenFlag=False, func=misclassifcation_error)\n",
    "print(accuracy_report)\n",
    "print(class_report)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No of questions = 1500\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "{'ABBR': 0.6666666666666666, 'DESC': 0.8260869565217391, 'ENTY': 0.7978723404255319, 'HUM': 0.8461538461538461, 'LOC': 0.691358024691358, 'NUM': 0.7787610619469026}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.86      0.67      0.75         9\n",
      "        DESC       0.77      0.83      0.79       138\n",
      "        ENTY       0.56      0.80      0.66        94\n",
      "         HUM       0.92      0.85      0.88        65\n",
      "         LOC       0.92      0.69      0.79        81\n",
      "         NUM       0.98      0.78      0.87       113\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.83      0.77      0.79       500\n",
      "weighted avg       0.82      0.79      0.80       500\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "accuracy_report, class_report, root, prediction, actual = getReport(train_data=data, test_data=test_data, lenFlag=False, posFlag=False)\n",
    "print(accuracy_report)\n",
    "print(class_report)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No of questions = 1000\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "{'ABBR': 0.6666666666666666, 'DESC': 0.9782608695652174, 'ENTY': 0.6276595744680851, 'HUM': 0.8461538461538461, 'LOC': 0.654320987654321, 'NUM': 0.7699115044247787}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.86      0.67      0.75         9\n",
      "        DESC       0.73      0.98      0.84       138\n",
      "        ENTY       0.60      0.63      0.61        94\n",
      "         HUM       0.87      0.85      0.86        65\n",
      "         LOC       0.88      0.65      0.75        81\n",
      "         NUM       1.00      0.77      0.87       113\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.82      0.76      0.78       500\n",
      "weighted avg       0.81      0.79      0.79       500\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "accuracy_report, class_report, root, prediction, actual = getReport(train_data=data, test_data=test_data, lenFlag=False, posFlag=False, func=entropy)\n",
    "print(accuracy_report)\n",
    "print(class_report)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No of questions = 1000\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "{'ABBR': 0.6666666666666666, 'DESC': 0.427536231884058, 'ENTY': 0.648936170212766, 'HUM': 0.8769230769230769, 'LOC': 0.6296296296296297, 'NUM': 0.7699115044247787}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.86      0.67      0.75         9\n",
      "        DESC       0.57      0.43      0.49       138\n",
      "        ENTY       0.35      0.65      0.45        94\n",
      "         HUM       0.93      0.88      0.90        65\n",
      "         LOC       0.82      0.63      0.71        81\n",
      "         NUM       0.97      0.77      0.86       113\n",
      "\n",
      "    accuracy                           0.64       500\n",
      "   macro avg       0.75      0.67      0.69       500\n",
      "weighted avg       0.71      0.64      0.66       500\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "accuracy_report, class_report, root, prediction, actual = getReport(train_data=data, test_data=test_data, lenFlag=False, posFlag=False, func=misclassifcation_error)\n",
    "print(accuracy_report)\n",
    "print(class_report)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No of questions = 1000\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "{'ABBR': 0.6666666666666666, 'DESC': 0.8188405797101449, 'ENTY': 0.7340425531914894, 'HUM': 0.8, 'LOC': 0.654320987654321, 'NUM': 0.7876106194690266}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.86      0.67      0.75         9\n",
      "        DESC       0.75      0.82      0.78       138\n",
      "        ENTY       0.50      0.73      0.60        94\n",
      "         HUM       0.96      0.80      0.87        65\n",
      "         LOC       0.88      0.65      0.75        81\n",
      "         NUM       0.98      0.79      0.87       113\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.82      0.74      0.77       500\n",
      "weighted avg       0.81      0.76      0.77       500\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Error Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_wrong_prediction(prediction, actual, dataset):\n",
    "    data_list = [dataset[i] for i in range(len(prediction)) if prediction[i] != actual[i]]\n",
    "    return data_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_ , class_matrix, root_gini, prediction_gini, actual_gini  = getReport(train_data=data, test_data=test_data)\n",
    "wrong_data = get_wrong_prediction(prediction_gini, actual_gini, test_data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No of questions = 1501\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Printing the wrong data length\n",
    "print('Len of wrong data for gini', len(wrong_data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Len of wrong data for gini 88\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_ , class_matrix, root_entropy, prediction_entropy, actual_entropy  = getReport(train_data=data, test_data=wrong_data, func=entropy)\n",
    "wrong_data_en = get_wrong_prediction(prediction_entropy, actual_entropy, wrong_data)\n",
    "print('Len of wrong data for entropy is', len(wrong_data_en))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No of questions = 1501\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "Len of wrong data for entropy is 78\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_ , class_matrix, root_mis, prediction_mis, actual_mis  = getReport(train_data=data, test_data=wrong_data, func=misclassifcation_error)\n",
    "wrong_data_mis = get_wrong_prediction(prediction_entropy, actual_entropy, wrong_data)\n",
    "print('Len of wrong data for misclassifcation_error is', len(wrong_data_mis))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No of questions = 1501\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "Len of wrong data for misclassifcation_error is 78\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Entropy correctly classifies', (len(wrong_data) - len(wrong_data_en)), 'as compared to GINI metric')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Entropy correctly classifies 10 as compared to GINI metric\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Misclassification error correctly classifies', (len(wrong_data) - len(wrong_data_mis)), 'as compared to GINI metric')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Misclassification error correctly classifies 10 as compared to GINI metric\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "3cb8c012cfbb4bd134dbec866004973183fdd2df9a4855f830bc1223b354bc0c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}